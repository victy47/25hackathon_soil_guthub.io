import requests
import re
import pandas as pd
from datetime import datetime
import time

class NaverNewsAPI:
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.api_url = "https://openapi.naver.com/v1/search/news.json"
    
    def search_news(self, query, display=100, start=1, sort="date"):
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        params = {
            "query": query,
            "display": display,
            "start": start,
            "sort": sort
        }
        
        try:
            response = requests.get(self.api_url, headers=headers, params=params)
            if response.status_code == 401:
                print("âŒ API í‚¤ ì¸ì¦ ì‹¤íŒ¨!")
                return None
            elif response.status_code == 403:
                print("âŒ API ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ!")
                return None
            elif response.status_code != 200:
                print(f"âŒ API ì˜¤ë¥˜: {response.status_code}")
                return None
            
            result = response.json()
            if start == 1:
                total = result.get('total', 0)
                print(f"âœ… API ì—°ê²° ì„±ê³µ! ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {total}ê°œ")
            
            return result
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return None
    
    def search_news_by_period(self, include_keywords, exclude_keywords=None,
                             start_date=None, end_date=None, max_results=1000):
        
        print(f"í¬í•¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {include_keywords}")
        if exclude_keywords:
            print(f"ì œì™¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {exclude_keywords}")
        if start_date and end_date:
            print(f"ê¸°ê°„: {start_date} ~ {end_date}")
        print("=" * 60)
        
        filter_by_date = bool(start_date and end_date)
        if filter_by_date:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(hour=23, minute=59, second=59)
        
        # 1ë‹¨ê³„: í‚¤ì›Œë“œë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘ (exclude í•„í„°ë§ ì—†ì´)
        all_articles = []
        
        for kw in include_keywords:
            print(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: {kw}")
            start_pos = 1
            
            while start_pos <= max_results:
                print(f"   ê²€ìƒ‰ ì¤‘... (ìœ„ì¹˜: {start_pos})")
                result = self.search_news(kw, display=100, start=start_pos, sort="date")
                
                if not result or 'items' not in result:
                    break
                
                items = result['items']
                if not items:
                    break
                
                for item in items:
                    try:
                        pub_date_str = item['pubDate']
                        pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
                        pub_date_naive = pub_date.replace(tzinfo=None)
                        
                        # ë‚ ì§œ í•„í„°ë§ë§Œ ì ìš©
                        if filter_by_date:
                            if pub_date_naive < start_dt or pub_date_naive > end_dt:
                                continue
                        
                        title = re.sub(r'<[^>]+>', '', item.get('title', ''))
                        description = re.sub(r'<[^>]+>', '', item.get('description', ''))
                        
                        all_articles.append({
                            'ì œëª©': title,
                            'ë‚´ìš©': description,
                            'ë§í¬': item.get('link'),
                            'ë°œí–‰ì¼': pub_date_naive.strftime("%Y-%m-%d %H:%M:%S"),
                            'ì›ë³¸ë§í¬': item.get('originallink', '')
                        })
                    except Exception as e:
                        continue
                
                start_pos += 100
                if start_pos > 1000:
                    print("   â†’ API ê²€ìƒ‰ í•œê³„ ë„ë‹¬ (start > 1000)")
                    break
                
                time.sleep(0.1)
        
        print(f"\nğŸ“Š 1ë‹¨ê³„ ì™„ë£Œ: ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ {len(all_articles)}ê°œ")
        
        # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
        seen = set()
        unique_articles = []
        for art in all_articles:
            link = art.get('ë§í¬')
            if link and link not in seen:
                seen.add(link)
                unique_articles.append(art)
        
        print(f"ğŸ“Š 2ë‹¨ê³„ ì™„ë£Œ: ì¤‘ë³µ ì œê±° í›„ {len(unique_articles)}ê°œ")
        
        # 3ë‹¨ê³„: ì œì™¸ í‚¤ì›Œë“œ í•„í„°ë§
        if exclude_keywords:
            filtered_articles = []
            for art in unique_articles:
                title = art.get('ì œëª©', '')
                description = art.get('ë‚´ìš©', '')
                
                # ì œì™¸ í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ë‚´ìš©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                has_exclude = any(exc in title or exc in description for exc in exclude_keywords)
                
                if not has_exclude:
                    filtered_articles.append(art)
            
            print(f"ğŸ“Š 3ë‹¨ê³„ ì™„ë£Œ: ì œì™¸ í‚¤ì›Œë“œ í•„í„°ë§ í›„ {len(filtered_articles)}ê°œ")
            return filtered_articles
        
        print(f"\nâœ… ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_articles)}ê°œ")
        return unique_articles
    
    def save_to_csv(self, articles, filename):
        if articles:
            df = pd.DataFrame(articles)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ CSV ì €ì¥ ì™„ë£Œ: {filename}")
        else:
            print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ ì—†ìŒ")
    
    def save_to_excel(self, articles, filename):
        if articles:
            df = pd.DataFrame(articles)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ Excel ì €ì¥ ì™„ë£Œ: {filename}")
        else:
            print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ ì—†ìŒ")

if __name__ == "__main__":
    client_id = "ec_FtupnwjtWaskgBSsp"
    client_secret = "0rGrlHT_2Q"
    
    api = NaverNewsAPI(client_id, client_secret)
    
    include_keywords = ["ì—ìŠ¤ì˜¤ì¼", "ì—ì“°ì˜¤ì¼", "S-OIL", "íˆì¦ˆì•„ì§€", "Sì˜¤ì¼"]
    exclude_keywords = ["ì§„í•™ì‚¬", "í€µë¦¬í¬íŠ¸", "ì£¼ìš”ë‹¨ì‹ ", "ì„ìœ ì™€ê°€ìŠ¤ì—…ì¢…", "í†¡í†¡ìƒí™œì •ë³´", "ì£¼ê°€ì •ë³´", "ë¶€ê³ ", "ë³„ì„¸", "ì„ì§ì›í‰ê· ì—°ë´‰"]
    
    start_date = "2025-11-01"
    end_date = "2025-11-20"
    
    articles = api.search_news_by_period(
        include_keywords=include_keywords,
        exclude_keywords=exclude_keywords,
        start_date=start_date,
        end_date=end_date,
        max_results=1000
    )
    
    if articles:
        api.save_to_csv(articles, "news_results.csv")
    